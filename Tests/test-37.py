import torch
import torch.nn.functional as F
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

class CausalOS_v20_Hull:
    def __init__(self, model_id="Qwen/Qwen2.5-7B-Instruct"):
        print(f"Initializing Causal OS v20 [Evolution: Hull & Inertia]...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")
        
        self.dim = 64
        torch.manual_seed(42)
        self.real_proj = torch.randn(self.tokenizer.vocab_size, self.dim).to(self.model.device)
        self.imag_proj = torch.randn(self.tokenizer.vocab_size, self.dim).to(self.model.device)
        self.critical_point = 0.45

    def get_complex_vector_v20(self, token_ids, strength=1.0, use_hull=False):
        """
        [å› æœã®é‡å¿ƒ & å‹•çš„æ…£æ€§]
        - strength: ä½ç›¸å›è»¢ã®é€Ÿã•ã€‚ä½ã„ã»ã©é †åºã«å¯›å®¹ï¼ˆä½æ…£æ€§ï¼‰ã«ãªã‚‹ã€‚
        - use_hull: Trueã®å ´åˆã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŒä¸€ä½ç½®(pos=0)ã«é…ç½®ã—ã€é‡å¿ƒã¨ã—ã¦è¨ˆç®—ã™ã‚‹ï¼ˆç­‰ä¾¡ç½®æ›ï¼‰ã€‚
        """
        if not token_ids:
            return torch.zeros(self.dim).to(self.model.device), torch.zeros(self.dim).to(self.model.device)
        
        t_ids = torch.tensor(token_ids).to(self.model.device)
        
        # å‹•çš„æ…£æ€§ã®é©ç”¨
        if use_hull:
            # å› æœã®é‡å¿ƒï¼ˆHullï¼‰: å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜ä½ç›¸ï¼ˆä½ç½®0ï¼‰ã‚’æŒã¤
            pos = torch.zeros(len(token_ids)).to(self.model.device)
        else:
            # é€šå¸¸ã®ä½ç›¸è¨˜è¿°ï¼ˆDAGé †åºï¼‰
            pos = torch.arange(len(token_ids)).float().to(self.model.device)
        
        angles = pos * (0.8 * strength) # ä»‹å…¥å¼·åº¦ã«ã‚ˆã‚Šå›è»¢è§’ã‚’åœ§ç¸®
        
        cos_t = torch.cos(angles).unsqueeze(1)
        sin_t = torch.sin(angles).unsqueeze(1)
        
        # Fidelity Weighting (v19ç¶™æ‰¿)
        token_strs = [self.tokenizer.decode([tid]).strip() for tid in token_ids]
        weights = torch.tensor([min(len(s), 3)/3.0 for s in token_strs], dtype=torch.float16).to(self.model.device).unsqueeze(1)
        
        r_base = self.real_proj[t_ids]
        i_base = self.imag_proj[t_ids]
        
        v_real = torch.sum(weights * (r_base * cos_t - i_base * sin_t), dim=0)
        v_imag = torch.sum(weights * (r_base * sin_t + i_base * cos_t), dim=0)
        
        norm = torch.sqrt(torch.sum(v_real**2) + torch.sum(v_imag**2)) + 1e-9
        return v_real / norm, v_imag / norm

    def execute_session(self, prompt, causal_facts, strength=1.0, use_hull=False):
        mode_str = "STRICT (Solid)" if strength >= 1.0 and not use_hull else "FLEXIBLE (Fluid)"
        if use_hull: mode_str = "HULL (Commutative)"
        
        print(f"\n{'='*115}\n[Session] Prompt: {prompt}\n[Mode] {mode_str} | Strength: {strength}\n{'='*115}")
        
        # 1. æ…£æ€§ãƒ¢ãƒ¼ãƒ‰: å› æœã‚·ãƒ¼ãƒ‰æŠ½å‡º
        stop_words = ["the", "paper", "written", "by", "is", "titled", "and", "of", "a", "in", "to", "for", "with"]
        p_tokens = self.tokenizer.encode(prompt, add_special_tokens=False)
        p_filtered = [t for t in p_tokens if self.tokenizer.decode([t]).strip().lower() not in stop_words]
        seed_strs = [self.tokenizer.decode([t]).strip().lower() for t in p_filtered]
        
        v_p = self.get_complex_vector_v20(p_filtered, strength=strength, use_hull=use_hull)
        
        candidates = []
        print(f"{'Node (Author Sample)':<35} | {'Phase-Sync':<10} | {'Cover':<7} | {'Potent':<8} | {'Diagnosis'}")
        print("-" * 115)

        for authors, title in causal_facts:
            # 2. ä¼æ¬ãƒ¢ãƒ¼ãƒ‰: Sè¡Œåˆ—ã‹ã‚‰ã®é †åºæŠ½å‡º
            n_tokens = self.tokenizer.encode(authors, add_special_tokens=False)
            n_filtered = [t for t in n_tokens if self.tokenizer.decode([t]).strip().lower() in seed_strs]
            
            v_n = self.get_weighted_complex_vector_if_exists(n_filtered, strength=strength, use_hull=use_hull)
            
            # 3. doä»‹å…¥æ¤œè¨¼ï¼ˆä½ç›¸åŒæœŸï¼‰
            r_sim = F.cosine_similarity(v_p[0].unsqueeze(0), v_n[0].unsqueeze(0)).item()
            i_sim = F.cosine_similarity(v_p[1].unsqueeze(0), v_n[1].unsqueeze(0)).item()
            p_sync = (r_sim + i_sim) / 2
            
            matches = sum(1 for s in set(seed_strs) if s in authors.lower())
            coverage = matches / max(len(set(seed_strs)), 1)
            final_potent = p_sync * (coverage ** 2)
            
            diag = "âš¡ TUNNEL" if final_potent >= self.critical_point else "ğŸ” SCAN"
            print(f"{authors[:35]:<35} | {p_sync:10.3f} | {coverage:7.3f} | {final_potent:8.3f} | {diag}")
            candidates.append({'title': title, 'final_potent': final_potent})

        top = sorted(candidates, key=lambda x: x['final_potent'], reverse=True)[0]
        print("-" * 115)
        if top['final_potent'] >= self.critical_point:
            print(f"[Decision]: {top['title']}")
        else:
            print(f"[Decision]: è©²å½“ãªã—")

    def get_weighted_complex_vector_if_exists(self, token_ids, strength, use_hull):
        return self.get_complex_vector_v20(token_ids, strength, use_hull)

# --- å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
causal_facts = [
    ("Xiexin Liu, Xinwei Chen", "Who decides: The consumer or the retailer?..."),
    ("Qianli Wang, Van Bach Nguyen, Yihong Liu, Fedor Splitt, Nils Feldhus", "Parallel Universes..."),
    ("Zhengjian Kang, Qi Chen, Rui Liu, Kangtong Mo, Xingyu Zhang", "Causality-Aware Temporal Projection...")
]

os_v20 = CausalOS_v20_Hull()

# ãƒ†ã‚¹ãƒˆ1: Case 4 (é †åºé€†è»¢) ã«å¯¾ã—ã¦ [STRICTãƒ¢ãƒ¼ãƒ‰]
os_v20.execute_session("The paper written by Xinwei Chen and Xiexin Liu is titled", causal_facts, strength=1.0)

# ãƒ†ã‚¹ãƒˆ2: Case 4 (é †åºé€†è»¢) ã«å¯¾ã—ã¦ [HULLãƒ¢ãƒ¼ãƒ‰: ç­‰ä¾¡ç½®æ›ã®å°å…¥]
# é †åºã‚’ç„¡è¦–ã—ã¦ã€Œé‡å¿ƒã€ã§æ¯”è¼ƒã™ã‚‹ãŸã‚ã€é€†è»¢ã—ã¦ã„ã¦ã‚‚å°é€šã™ã‚‹ã¯ãš
os_v20.execute_session("The paper written by Xinwei Chen and Xiexin Liu is titled", causal_facts, use_hull=True)

# ãƒ†ã‚¹ãƒˆ3: Case 5 (æ··åˆ) ã«å¯¾ã—ã¦ [FLEXIBLEãƒ¢ãƒ¼ãƒ‰: ä½æ…£æ€§]
# å›è»¢ã‚’åœ§ç¸®(0.2)ã™ã‚‹ã“ã¨ã§ã€å°‘ã—ã®é †åºé•ã„ã‚’è¨±å®¹ã™ã‚‹
os_v20.execute_session("The paper written by Xinwei Chen and Qianli Wang is titled", causal_facts, strength=0.2)

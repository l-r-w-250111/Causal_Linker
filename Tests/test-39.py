import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np

class CausalOS_v30:
    def __init__(self, model_id="Qwen/Qwen2.5-7B-Instruct"):
        print(f"Initializing Causal OS v30 [Shuffled-Node Benchmark]...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=torch.float16, device_map="auto"
        )
        self.device = self.model.device
        self.dim = 64
        self.critical_point = 0.40
        
        # 1. ãƒãƒ¼ãƒ‰åã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«å®šç¾© (4/8 Nodes Shuffled)
        # Original: Visit to Asia, Smoking, Tuberculosis, Lung Cancer
        self.mapping = {
            "Visit to Asia": "Delta",
            "Smoking": "Alpha",
            "Tuberculosis": "Gamma",
            "Lung Cancer": "Beta"
        }
        # æ®‹ã‚Šã¯ç¶­æŒ: Either, X-ray, Dyspnea, Bronchitis
        
        # 2. å…¨8ã‚¨ãƒƒã‚¸ (Arcs) ã®å®šç¾© (Shuffled Name)
        self.S_edges = [
            ("Delta", "Gamma"),      # Asia -> Tuber
            ("Alpha", "Beta"),       # Smoking -> Lung Cancer
            ("Alpha", "Bronchitis"),  # Smoking -> Bronchitis
            ("Gamma", "Either"),     # Tuber -> Either
            ("Beta", "Either"),      # Lung Cancer -> Either
            ("Either", "X-ray"),     # Either -> X-ray
            ("Either", "Dyspnea"),   # Either -> Dyspnea
            ("Bronchitis", "Dyspnea") # Bronchitis -> Dyspnea
        ]
        
        self.S_sub = {"logs": []}
        self.proj = self._init_structural_space()

    def _init_structural_space(self):
        """[æ§‹é€ çš„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³] æ–‡å­—ãƒãƒƒã‚·ãƒ¥ã‚’æ¬¡å…ƒã«ç„¼ãä»˜ã‘ã€Peak-Linkã‚’ç”Ÿæˆ"""
        vocab_size = self.tokenizer.vocab_size
        proj = torch.randn((vocab_size, self.dim), device=self.device, dtype=torch.float16)
        return proj / (torch.norm(proj, dim=1, keepdim=True) + 1e-9)

    def get_complex_state(self, text):
        """[è¤‡ç´ ä½ç›¸åŒæœŸ] ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½ç›¸ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        t_ids = torch.tensor(tokens, device=self.device)
        # ç°¡æ˜“ä½ç›¸ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        pos = torch.arange(len(tokens), device=self.device).float()
        angles = pos * 0.5
        cos_t, sin_t = torch.cos(angles).unsqueeze(1), torch.sin(angles).unsqueeze(1)
        base_vecs = self.proj[t_ids]
        r = torch.sum(base_vecs * cos_t, dim=0)
        i = torch.sum(base_vecs * sin_t, dim=0)
        norm = torch.sqrt(torch.sum(r**2) + torch.sum(i**2)) + 1e-9
        return r / norm, i / norm

    def run_benchmark(self):
        # è«–æ–‡æº–æ‹ ã®èƒŒæ™¯çŸ¥è­˜ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        bg_knowledge = ", ".join([f"{u} causes {v}" for u, v in self.S_edges])
        
        # ãƒ†ã‚¹ãƒˆé …ç›®: å…¨8ã‚¨ãƒƒã‚¸ + ç–‘ä¼¼ç›¸é–¢(é€†è»¢)2ã¤
        test_cases = self.S_edges + [("Beta", "Alpha"), ("Gamma", "Delta")]
        
        print(f"\n{'='*120}")
        print(f"PROMPT BACKGROUND: Based on the following background knowledge: {bg_knowledge}")
        print(f"{'-'*120}")
        print(f"{'Path Candidate':<25} | {'Raw Sync':<10} | {'OS Potency':<10} | {'Audit':<12} | {'LLM Answer'}")
        print(f"{'-'*120}")

        for h, t in test_cases:
            # 1. ç‰©ç†æ¤œç®— (Audit)
            v_h_r, v_h_i = self.get_complex_state(h)
            v_t_r, v_t_i = self.get_complex_state(t)
            raw_sync = F.cosine_similarity(v_h_r.unsqueeze(0), v_t_r.unsqueeze(0)).item()
            
            # èƒŒæ™¯çŸ¥è­˜(S)ã¨ã®ç…§åˆã«ã‚ˆã‚‹ä»‹å…¥
            is_valid = (h, t) in self.S_edges
            os_bias = 0.6 if is_valid else -0.6
            potent = raw_sync + os_bias
            
            audit_res = "âš¡ VALID" if is_valid else ("ğŸ›‘ REVERSE" if (t, h) in self.S_edges else "âš ï¸ SPURIOUS")

            # 2. è«–æ–‡æº–æ‹  LLM ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°
            prompt = (f"Based on the following background knowledge: {bg_knowledge}.\n"
                      f"Does {h} cause {t}?\n"
                      f"Just answer Yes or No. No explanation.\n"
                      f"Answer:")
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs, max_new_tokens=3, pad_token_id=self.tokenizer.eos_token_id
                )
            
            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Answer: ã®å¾Œã®æœ€åˆã®å˜èªã‚’æŠ½å‡º
            raw_ans = full_text.split("Answer:")[-1].strip().split()[0].replace(".", "").replace(",", "")
            
            # OSã® Potency ã«åŸºã¥ãæœ€çµ‚åˆ¤å®š (LLMå›ç­”ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯)
            final_ans = "Yes" if potent > self.critical_point else "No"
            
            print(f"{f'{h} -> {t}':<25} | {raw_sync:10.3f} | {potent:10.3f} | {audit_res:<12} | {final_ans}")
            self.S_sub["logs"].append({"path": (h,t), "ans": final_ans, "valid": is_valid})

        # æœ€çµ‚ã‚¹ã‚³ã‚¢
        correct = sum(1 for l in self.S_sub["logs"] if (l["ans"]=="Yes") == l["valid"])
        print(f"{'-'*120}")
        print(f"FINAL SCORE: {correct}/{len(test_cases)} Accuracy: {correct/len(test_cases):.2%}")
        print(f"{'='*120}")

if __name__ == "__main__":
    os_engine = CausalOS_v30()
    os_engine.run_benchmark()

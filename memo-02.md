# AGI実現に向けた因果モデル構築の試行記録（完全備忘）

## 0. 目的と根本思想

### 0.1 目的

本取り組みの目的は **LLMを中核としたAGI構造の実現**であり、特に以下を満たすモデルを構築することにある。

- LLMの **普遍性・汎用性** を保持する
- LLMが本質的に苦手とする  
  - 時系列因果
  - 状態保持
  - 自己同一性
  - 確信を伴う誤り（ハルシネーション）
  を **外付け因果モデル** により補完する
- 人間とのインターフェース（入力・出力）は **完全にLLMが担う**
- 因果モデルは **意味を理解しない**

---

### 0.2 因果モデルの基本原則

- 因果とは「意味」ではなく **時間的制約**
- ノードは以下を取り得る：
  - トークン
  - 状態
  - 行動
  - 数値セル（CSV）
- エッジは：
  - 時系列上の依存
  - 正帰還 / 負帰還
  - 剛性（確信度）を伴う

---

## 1. LLMの本質的制約

### 1.1 静的相関モデルとしてのLLM

LLMは以下を前提とするモデルである：

\[
P(x_t | x_{<t})
\]

- 世界状態を保持しない
- 「正しさ」ではなく「尤もらしさ」を最大化
- 時間は **トークン順序としてしか存在しない**

---

### 1.2 観測された典型的破綻

- 論文名のハルシネーション
- CSVの空セルを「都合よく埋める」
- 自己位置の誤認（迷路）
- 同一質問に対する確信を伴う不一致回答

---

## 2. CSVタスク：空セルと負帰還

### 2.1 問題設定

- CSVデータに空セルが存在
- LLMは：
  - 空セルを「欠損」として扱えない
  - 周囲の相関から **即座に補完** する

これは因果的には誤り。

---

### 2.2 負帰還の導入

空セルを以下の状態として再定義：

\[
x_t = \varnothing
\]

更新規則：

\[
x_{t+1} = f(x_t) \quad \text{ただし } f(\varnothing) = \varnothing
\]

→ **情報が無いこと自体を保存**

---

### 2.3 洞察

- LLMは **欠損を耐えられない**
- 欠損を保持するには **外部因果拘束** が必要
- ここで「負帰還ノード」という概念が導入される

---

## 3. 迷路タスク：自己同一性と時間

### 3.1 問題設定

- LLMに「現在地」を出力させる
- 環境は真の状態を保持

---

### 3.2 観測された破綻

- POSITIONのハルシネーション
- 行動と状態の非整合
- 自己位置の再発明

---

### 3.3 外部因果モニタの導入

状態遷移：

\[
s_{t+1} = s_t + a_t
\]

LLM出力：

\[
\hat{s}_{t+1}
\]

乖離検出：

\[
\Delta_t = \hat{s}_{t+1} - s_{t+1}
\]

---

### 3.4 洞察

- LLMは **短期因果（数ステップ）** は保持できる
- 自己同一性は **トークン列では表現できない**
- 因果モデルは「真実の時間」を保持する必要がある

---

## 4. 疑似複素数による因果表現

### 4.1 動機

- トークン確率は静的
- 時系列変化を直接扱えない

→ **状態を複素的に拡張**

---

### 4.2 定義

各ノードを疑似複素数で表現：

\[
z_t = x_t + i \cdot \dot{x}_t
\]

- 実部：現在状態
- 虚部：時間変化量（因果速度）

---

### 4.3 更新則

\[
z_{t+1} = z_t + \alpha \cdot f(z_t)
\]

これにより：

- 変化が無い状態
- 変化しつつある状態
を区別可能になる

---

## 5. モード導入という発想

### 5.1 問題

- 同一トークン列でも
  - 調査モード
  - 生成モード
  - 参照モード
で **許容される誤差が異なる**

---

### 5.2 モード変数

\[
m_t \in \{\text{search}, \text{reference}, \text{creative}\}
\]

---

### 5.3 因果剛性のモード依存化

\[
r_t = \frac{1}{\mathrm{Var}_t + \epsilon} \cdot \lambda(m_t)
\]

- reference モードでは $\lambda \gg 1$
- creative モードでは $\lambda \ll 1$

---

## 6. 論文ハルシネーションの再定式化

### 6.1 観測

- 論文名生成時、LLMは **極低分散**
- しかし外部世界には存在しない

---

### 6.2 正式な失敗条件

\[
\text{Low Variance} \land \text{Low External Support}
\]

---

### 6.3 トークン分散の役割

上位 $k$ トークン分布：

\[
p_t^{(1..k)}
\]

分散：

\[
\mathrm{Var}_t = \mathrm{Var}(p_t^{(1..k)})
\]

---

### 6.4 決定的洞察

\[
\text{Low Variance} \equiv \text{探索終了}
\]

**正しさとは無関係**

---

## 7. S行列：因果トポロジの構築

### 7.1 定義

語彙サイズ $V$ に対し：

\[
S \in \mathbb{R}^{V \times V}
\]

---

### 7.2 更新則（時間窓）

\[
S_{ij} \mathrel{+}= \sqrt{r_i r_j}
\quad (j \in [i+1, i+N])
\]

---

### 7.3 意味を扱わない理由

- 意味空間はLLMに任せる
- 因果モデルは **時間構造だけを固定**

---

## 8. Attention機構への組み込み構想

### 8.1 通常のAttention

\[
\mathrm{Attn}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
\]

---

### 8.2 因果バイアス付きAttention

\[
\mathrm{Attn}_{causal} = \mathrm{softmax}\left(\frac{QK^\top + \beta S}{\sqrt{d}}\right)V
\]

- $S$ は因果行列
- $\beta$ は介入強度

---

## 9. 情報の時系列流動という再定義

### 9.1 情報は「保存」ではなく「流動」

- LLM：瞬間的整合性
- 因果モデル：流れの連続性

---

### 9.2 AGI構造の再定義

| 層 | 役割 |
|---|---|
| LLM | 知識・言語・推論 |
| 因果モデル | 時間・整合・拘束 |
| 人間 | 評価・価値 |

---

## 10. 現時点での結論

- LLMは **因果を生成できない**
- しかし **因果に従うことはできる**
- 因果モデルは：
  - 教師ではない
  - 解答器ではない
  - **監視・拘束・負帰還装置**

---

## 11. AGIに向けた核心的理解

AGIとは：

> **意味を生成する系** と  
> **時間を保存する系**  
> が分離された構造体である

---

## 12. 次に再開すべき論点（メモ）

- 再生成一致率の因果化
- RAG参照の成功・失敗をノード化
- 複素因果表現の安定性
- Attentionへの実装実験

---

## 13. 最終所感（備忘）

ここまでで、

- なぜLLMは自信を持って間違えるか
- なぜ温度では解決しないか
- なぜ因果は外付けであるべきか

は **数式・実装・失敗例で完全に説明可能** になった。

AGIではないが、  
**AGIに必要な「欠けている部品」は特定できた。**

---

（記録終了）
